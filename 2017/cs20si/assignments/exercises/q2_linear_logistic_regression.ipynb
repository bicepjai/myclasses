{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.client import device_lib\n",
    "print(tf.__version__)\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 462 entries, 0 to 461\n",
      "Data columns (total 10 columns):\n",
      "sbp          462 non-null int64\n",
      "tobacco      462 non-null float64\n",
      "ldl          462 non-null float64\n",
      "adiposity    462 non-null float64\n",
      "famhist      462 non-null object\n",
      "typea        462 non-null int64\n",
      "obesity      462 non-null float64\n",
      "alcohol      462 non-null float64\n",
      "age          462 non-null int64\n",
      "chd          462 non-null int64\n",
      "dtypes: float64(5), int64(4), object(1)\n",
      "memory usage: 36.2+ KB\n",
      "None\n",
      "['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age', 'chd']\n",
      "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n",
      "0  160    12.00  5.73      23.11  Present     49    25.30    97.20   52    1\n",
      "1  144     0.01  4.41      28.61   Absent     55    28.87     2.06   63    1\n",
      "2  118     0.08  3.48      32.28  Present     52    29.14     3.81   46    0\n",
      "3  170     7.50  6.41      38.03  Present     51    31.99    24.26   58    1\n",
      "4  134    13.60  3.50      27.78  Present     60    25.99    57.34   49    1\n"
     ]
    }
   ],
   "source": [
    "heart_data = pd.read_csv(\"../../data/heart.csv\")\n",
    "print heart_data.info()\n",
    "print [name for name in heart_data.columns]\n",
    "print heart_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on heart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "DATA_PATH = '../../data/heart.csv'\n",
    "N_FEATURES = 9\n",
    "\n",
    "EPOCHS = 1000\n",
    "LEARNING_RATE=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: get data\n",
    "\n",
    "def batch_generator(filenames):\n",
    "    \"\"\" filenames is the list of files you want to read from. \n",
    "    In this case, it contains only heart.csv\n",
    "    \"\"\"\n",
    "    filename_queue = tf.train.string_input_producer(filenames)\n",
    "    reader = tf.TextLineReader(skip_header_lines=1) # skip the first line in the file\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    # record_defaults are the default values in case some of our columns are empty\n",
    "    # This is also to tell tensorflow the format of our data (the type of the decode result)\n",
    "    # for this dataset, out of 9 feature columns, \n",
    "    # 8 of them are floats (some are integers, but to make our features homogenous, \n",
    "    # we consider them floats), and 1 is string (at position 5)\n",
    "    # the last column corresponds to the lable is an integer\n",
    "\n",
    "    record_defaults = [[1.0] for _ in range(N_FEATURES)]\n",
    "    record_defaults[4] = ['']\n",
    "    record_defaults.append([1])\n",
    "\n",
    "    # read in the 10 rows of data\n",
    "    content = tf.decode_csv(value, record_defaults=record_defaults) \n",
    "\n",
    "    # convert the 5th column (present/absent) to the binary value 0 and 1\n",
    "    condition = tf.equal(content[4], tf.constant('Present'))\n",
    "    content[4] = tf.cond(condition, lambda: tf.constant(1.0), lambda: tf.constant(0.0))\n",
    "\n",
    "    # pack all 9 features into a tensor\n",
    "    features = tf.stack(content[:N_FEATURES])\n",
    "\n",
    "    # assign the last column to label\n",
    "    label = content[-1]\n",
    "\n",
    "    # minimum number elements in the queue after a dequeue, used to ensure \n",
    "    # that the samples are sufficiently mixed\n",
    "    # I think 10 times the BATCH_SIZE is sufficient\n",
    "    min_after_dequeue = 10 * BATCH_SIZE\n",
    "\n",
    "    # the maximum number of elements in the queue\n",
    "    capacity = 20 * BATCH_SIZE\n",
    "\n",
    "    # shuffle the data to generate BATCH_SIZE sample pairs\n",
    "    data_batch, label_batch = tf.train.shuffle_batch([features, label], batch_size=BATCH_SIZE, \n",
    "                                        capacity=capacity, min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "    return data_batch, label_batch\n",
    "\n",
    "def generate_batches(data_batch, label_batch):\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for _ in range(10): # generate 10 batches\n",
    "            features, labels = sess.run([data_batch, label_batch])\n",
    "            print features\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "data1_feature_batch, data1_label_batch = batch_generator([DATA_PATH])\n",
    "# generate_batches(data_batch, label_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_row(csv_row):\n",
    "    record_defaults = [[0], [0.0], [0.0], [0.0], [\"\"], [0], [0.0], [0.0], [0], [0]]\n",
    "    row = tf.decode_csv(csv_row, record_defaults=record_defaults)\n",
    "    row[4] = tf.cond(tf.equal(row[4], \"Present\"), lambda: 1, lambda: 0)\n",
    "    return row[:-1], row[-1]\n",
    "\n",
    "def input_pipeline(filenames, batch_size):\n",
    "    # Define a `tf.contrib.data.Dataset` for iterating over one epoch of the data.\n",
    "    dataset = (tf.contrib.data.TextLineDataset(filenames)\n",
    "               .skip(1)\n",
    "               .map(lambda line: read_row(line))\n",
    "               .shuffle(buffer_size=10)  # Equivalent to min_after_dequeue=10.\n",
    "               .batch(batch_size))\n",
    "\n",
    "    # Return an *initializable* iterator over the dataset, which will allow us to\n",
    "    # re-initialize it at the beginning of each epoch.\n",
    "    return dataset.make_initializable_iterator()\n",
    "\n",
    "data_iterator = input_pipeline([DATA_PATH], BATCH_SIZE)\n",
    "data2_feature_batch, data2_label_batch = data_iterator.get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X_1:0\", shape=(?, 9), dtype=float32)\n",
      "Tensor(\"Y_1:0\", shape=(?,), dtype=float32)\n",
      "<tf.Variable 'weights_1:0' shape=(9, 1) dtype=float32_ref>\n",
      "<tf.Variable 'bias_1:0' shape=(1, 1) dtype=float32_ref>\n",
      "Tensor(\"add_2:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"Reshape_5:0\", shape=(1,), dtype=float32)\n",
      "name: \"GradientDescent_1\"\n",
      "op: \"NoOp\"\n",
      "input: \"^GradientDescent_1/update_weights_1/ApplyGradientDescent\"\n",
      "input: \"^GradientDescent_1/update_bias_1/ApplyGradientDescent\"\n",
      "\n",
      "Tensor(\"Softmax_1:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"Equal_2:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: create placeholders for input X (Features) and label Y (binary result)\n",
    "X = tf.placeholder(tf.float32, shape=[None, 9], name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, shape=[None], name=\"Y\")\n",
    "\n",
    "# Step 3: create weight and bias, initialized to 0\n",
    "w = tf.Variable(tf.truncated_normal([9, 1]), name=\"weights\")\n",
    "b = tf.Variable(tf.zeros([1,1]), name=\"bias\")\n",
    "\n",
    "# Step 4: logistic multinomial regression / softmax\n",
    "score = tf.matmul(X, w) + b\n",
    "\n",
    "# Step 5: define loss function\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=tf.transpose(score), labels=Y, name=\"loss\")\n",
    "\n",
    "# Step 6: using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "\n",
    "# Step 7: Prediction\n",
    "Y_predicted = tf.nn.softmax(tf.matmul(X, w) + b)\n",
    "correct_prediction = tf.equal(tf.argmax(Y,1), tf.argmax(Y_predicted,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(X)\n",
    "print(Y)\n",
    "print(w)\n",
    "print(b)\n",
    "\n",
    "print(score)\n",
    "print(loss)\n",
    "print(optimizer)\n",
    "print(Y_predicted)\n",
    "print(correct_prediction)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data1 run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1747.45it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    # Step 7: initialize the necessary variables, in this case, w and b\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    # Step 8: train the model\n",
    "    for i in tqdm(range(EPOCHS)): # run epochs\n",
    "        x_batch, y_batch = sess.run([data1_feature_batch, data1_label_batch])\n",
    "        # Session runs train_op to minimize loss\n",
    "        feed_dict={X: x_batch, Y:y_batch}\n",
    "        \n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "    # Step 9: output the values of w and b\n",
    "    w_value, b_value = sess.run([w, b])\n",
    "\n",
    "    # Step 10: predict\n",
    "    # print(sess.run(accuracy, feed_dict={X: mnist.validation.images, Y:mnist.validation.labels}))\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data2 run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [00:00<00:01, 473.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n"
     ]
    },
    {
     "ename": "OutOfRangeError",
     "evalue": "End of sequence\n\t [[Node: IteratorGetNext_1 = IteratorGetNext[output_shapes=[[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], output_types=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator_1)]]\n\nCaused by op u'IteratorGetNext_1', defined at:\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-55b9988e14f0>\", line 20, in <module>\n    data2_feature_batch, data2_label_batch = data_iterator.get_next()\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 247, in get_next\n    name=name))\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 254, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[Node: IteratorGetNext_1 = IteratorGetNext[output_shapes=[[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], output_types=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bd6aad76f7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Step 8: train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# run epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata2_feature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2_label_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata2_feature_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Session runs train_op to minimize loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[Node: IteratorGetNext_1 = IteratorGetNext[output_shapes=[[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], output_types=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator_1)]]\n\nCaused by op u'IteratorGetNext_1', defined at:\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-55b9988e14f0>\", line 20, in <module>\n    data2_feature_batch, data2_label_batch = data_iterator.get_next()\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 247, in get_next\n    name=name))\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 254, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[Node: IteratorGetNext_1 = IteratorGetNext[output_shapes=[[-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [-1]], output_types=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator_1)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Step 7: initialize the necessary variables, in this case, w and b\n",
    "    tf.global_variables_initializer().run()\n",
    "    sess.run(data_iterator.initializer)\n",
    "    \n",
    "    # Step 8: train the model\n",
    "    for i in tqdm(range(EPOCHS)): # run epochs\n",
    "        x_batch, y_batch = sess.run([data2_feature_batch, data2_label_batch])\n",
    "        print(np.shape(data2_feature_batch))\n",
    "        # Session runs train_op to minimize loss\n",
    "        feed_dict={X: np.transpose(x_batch), Y:y_batch}\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "    # Step 9: output the values of w and b\n",
    "    w_value, b_value = sess.run([w, b])\n",
    "\n",
    "    # Step 10: predict\n",
    "    # print(sess.run(accuracy, feed_dict={X: mnist.validation.images, Y:mnist.validation.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
