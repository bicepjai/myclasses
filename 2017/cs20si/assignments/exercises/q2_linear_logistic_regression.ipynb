{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'/gpu:0', u'/gpu:1']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.client import device_lib\n",
    "print(tf.__version__)\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 413 entries, 0 to 412\n",
      "Data columns (total 10 columns):\n",
      "sbp          413 non-null int64\n",
      "tobacco      413 non-null float64\n",
      "ldl          413 non-null float64\n",
      "adiposity    413 non-null float64\n",
      "famhist      413 non-null object\n",
      "typea        413 non-null int64\n",
      "obesity      413 non-null float64\n",
      "alcohol      413 non-null float64\n",
      "age          413 non-null int64\n",
      "chd          413 non-null int64\n",
      "dtypes: float64(5), int64(4), object(1)\n",
      "memory usage: 32.3+ KB\n",
      "None\n",
      "['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age', 'chd']\n",
      "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n",
      "0  160    12.00  5.73      23.11  Present     49    25.30    97.20   52    1\n",
      "1  144     0.01  4.41      28.61   Absent     55    28.87     2.06   63    1\n",
      "2  118     0.08  3.48      32.28  Present     52    29.14     3.81   46    0\n",
      "3  170     7.50  6.41      38.03  Present     51    31.99    24.26   58    1\n",
      "4  134    13.60  3.50      27.78  Present     60    25.99    57.34   49    1\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../../data/heart_train.csv\")\n",
    "test_data = pd.read_csv(\"../../data/heart_test.csv\")\n",
    "\n",
    "print train_data.info()\n",
    "print [name for name in train_data.columns]\n",
    "print train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on heart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../../data/heart_train.csv'\n",
    "TEST_DATA_PATH = '../../data/heart_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== training ===============\n",
      "413\n",
      "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n",
      "0  160    12.00  5.73      23.11  Present     49    25.30    97.20   52    1\n",
      "1  144     0.01  4.41      28.61   Absent     55    28.87     2.06   63    1\n",
      "2  118     0.08  3.48      32.28  Present     52    29.14     3.81   46    0\n",
      "3  170     7.50  6.41      38.03  Present     51    31.99    24.26   58    1\n",
      "4  134    13.60  3.50      27.78  Present     60    25.99    57.34   49    1\n",
      "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age\n",
      "0  160    12.00  5.73      23.11        1     49    25.30    97.20   52\n",
      "1  144     0.01  4.41      28.61        0     55    28.87     2.06   63\n",
      "2  118     0.08  3.48      32.28        1     52    29.14     3.81   46\n",
      "3  170     7.50  6.41      38.03        1     51    31.99    24.26   58\n",
      "4  134    13.60  3.50      27.78        1     60    25.99    57.34   49\n",
      "(413, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"=========== training ===============\")\n",
    "pd_train_data = pd.read_csv(TRAIN_DATA_PATH)\n",
    "n_train_data, _ = pd_train_data.shape\n",
    "print n_train_data\n",
    "\n",
    "print pd_train_data.head()\n",
    "pd_train_data[\"famhist\"] = pd_train_data[\"famhist\"].astype('category')\n",
    "pd_train_data[\"famhist\"] = pd_train_data[\"famhist\"].cat.codes\n",
    "train_labels = np.array(pd_train_data.chd)\n",
    "pd_train_data.drop(\"chd\", axis=1, inplace=True)\n",
    "print pd_train_data.head()\n",
    "train_data = np.float64(pd_train_data)\n",
    "print train_data.shape\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(train_data, train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== testing ===============\n",
      "49\n",
      "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n",
      "0  138     6.00  7.24      37.05   Absent     38    28.69     0.00   59    0\n",
      "1  148     0.00  5.32      26.71  Present     52    32.21    32.78   27    0\n",
      "2  124     4.20  2.94      27.59   Absent     50    30.31    85.06   30    0\n",
      "3  118     1.62  9.01      21.70   Absent     59    25.89    21.19   40    0\n",
      "4  116     4.28  7.02      19.99  Present     68    23.31     0.00   52    1\n",
      "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age\n",
      "0  138     6.00  7.24      37.05        0     38    28.69     0.00   59\n",
      "1  148     0.00  5.32      26.71        1     52    32.21    32.78   27\n",
      "2  124     4.20  2.94      27.59        0     50    30.31    85.06   30\n",
      "3  118     1.62  9.01      21.70        0     59    25.89    21.19   40\n",
      "4  116     4.28  7.02      19.99        1     68    23.31     0.00   52\n",
      "(49, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77551020408163263"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=========== testing ===============\")\n",
    "\n",
    "pd_test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "n_test_data, _ = pd_test_data.shape\n",
    "print n_test_data\n",
    "\n",
    "print pd_test_data.head()\n",
    "pd_test_data[\"famhist\"] = pd_test_data[\"famhist\"].astype('category')\n",
    "pd_test_data[\"famhist\"] = pd_test_data[\"famhist\"].cat.codes\n",
    "test_labels = np.array(pd_test_data.chd)\n",
    "pd_test_data.drop(\"chd\", axis=1, inplace=True)\n",
    "print pd_test_data.head()\n",
    "test_data = np.float64(pd_test_data)\n",
    "print test_data.shape\n",
    "\n",
    "test_prob = log_reg.predict(pd_test_data)\n",
    "log_reg.predict(test_data)\n",
    "log_reg.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "N_FEATURES = 9\n",
    "BETA = 1 # regularizer\n",
    "EPOCHS = 1000\n",
    "LEARNING_RATE=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 1: get data\n",
    "\n",
    "def batch_generator(filenames):\n",
    "    \"\"\" filenames is the list of files you want to read from. \n",
    "    In this case, it contains only heart.csv\n",
    "    \"\"\"\n",
    "    filename_queue = tf.train.string_input_producer(filenames)\n",
    "    reader = tf.TextLineReader(skip_header_lines=1) # skip the first line in the file\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    # record_defaults are the default values in case some of our columns are empty\n",
    "    # This is also to tell tensorflow the format of our data (the type of the decode result)\n",
    "    # for this dataset, out of 9 feature columns, \n",
    "    # 8 of them are floats (some are integers, but to make our features homogenous, \n",
    "    # we consider them floats), and 1 is string (at position 5)\n",
    "    # the last column corresponds to the lable is an integer\n",
    "\n",
    "    record_defaults = [[1.0] for _ in range(N_FEATURES)]\n",
    "    record_defaults[4] = ['']\n",
    "    record_defaults.append([1])\n",
    "\n",
    "    # read in the 10 rows of data\n",
    "    content = tf.decode_csv(value, record_defaults=record_defaults) \n",
    "\n",
    "    # convert the 5th column (present/absent) to the binary value 0 and 1\n",
    "    condition = tf.equal(content[4], tf.constant('Present'))\n",
    "    content[4] = tf.cond(condition, lambda: tf.constant(1.0), lambda: tf.constant(0.0))\n",
    "\n",
    "    # pack all 9 features into a tensor\n",
    "    features = tf.stack(content[:N_FEATURES])\n",
    "    # assign the last column to label\n",
    "    labels = tf.stack([1 - content[-1], content[-1]], 0)\n",
    "\n",
    "    # minimum number elements in the queue after a dequeue, used to ensure \n",
    "    # that the samples are sufficiently mixed\n",
    "    # I think 10 times the BATCH_SIZE is sufficient\n",
    "    min_after_dequeue = 10 * BATCH_SIZE\n",
    "\n",
    "    # the maximum number of elements in the queue\n",
    "    capacity = 20 * BATCH_SIZE\n",
    "\n",
    "    # shuffle the data to generate BATCH_SIZE sample pairs\n",
    "    data_batch, label_batch = tf.train.shuffle_batch([features, labels], batch_size=BATCH_SIZE, \n",
    "                                        capacity=capacity, min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "    return data_batch, label_batch\n",
    "\n",
    "def generate_batches(data_batch, label_batch):\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for _ in range(10): # generate 10 batches\n",
    "            features, labels = sess.run([data_batch, label_batch])\n",
    "            print features, labels\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "data1_feature_batch, data1_label_batch = batch_generator([TRAIN_DATA_PATH])\n",
    "test_data1_feature_batch, test_data1_label_batch = batch_generator([TEST_DATA_PATH])\n",
    "\n",
    "# generate_batches(data1_feature_batch, data1_label_batch)\n",
    "# generate_batches(test_data1_feature_batch, test_data1_label_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X', <tf.Tensor 'X_6:0' shape=(?, 9) dtype=float32>)\n",
      "('Y', <tf.Tensor 'Y_6:0' shape=(?, 2) dtype=float32>)\n",
      "('w', <tf.Variable 'weights_6:0' shape=(9, 2) dtype=float32_ref>)\n",
      "('b', <tf.Variable 'bias_6:0' shape=(1, 2) dtype=float32_ref>)\n",
      "('score', <tf.Tensor 'add_9:0' shape=(?, 2) dtype=float32>)\n",
      "('loss', <tf.Tensor 'loss_6:0' shape=() dtype=float32>)\n",
      "('optimizer', <tf.Operation 'GradientDescent_6' type=NoOp>)\n",
      "('Y_predicted', <tf.Tensor 'Softmax_4:0' shape=(?, 2) dtype=float32>)\n",
      "('correct_prediction', <tf.Tensor 'Equal_7:0' shape=(?,) dtype=bool>)\n",
      "('accuracy', <tf.Tensor 'Mean_4:0' shape=() dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: create placeholders for input X (Features) and label Y (binary result)\n",
    "X = tf.placeholder(tf.float32, shape=[None, 9], name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, shape=[None,2], name=\"Y\")\n",
    "\n",
    "# Step 3: create weight and bias, initialized to 0\n",
    "w = tf.Variable(tf.truncated_normal([9, 2]), name=\"weights\")\n",
    "b = tf.Variable(tf.zeros([1,2]), name=\"bias\")\n",
    "\n",
    "print(\"X\",X)\n",
    "print(\"Y\",Y)\n",
    "print(\"w\",w)\n",
    "print(\"b\",b)\n",
    "\n",
    "# Step 4: logistic multinomial regression / softmax\n",
    "score = tf.matmul(X, w) + b\n",
    "\n",
    "# Step 5: define loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=score, labels=Y, name=\"entropy\")\n",
    "\n",
    "regularizer = tf.nn.l2_loss(w)\n",
    "loss = tf.reduce_mean(entropy + BETA * regularizer, name=\"loss\")\n",
    "\n",
    "# Step 6: using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "\n",
    "print(\"score\",score)\n",
    "print(\"loss\", loss)\n",
    "print(\"optimizer\", optimizer)\n",
    "\n",
    "# Step 7: Prediction\n",
    "Y_predicted = tf.nn.softmax(tf.matmul(X, w) + b)\n",
    "correct_prediction = tf.equal(tf.argmax(Y_predicted,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"Y_predicted\", Y_predicted)\n",
    "print(\"correct_prediction\", correct_prediction)\n",
    "print(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This method did not workout\n",
    "https://stackoverflow.com/questions/37091899/how-to-actually-read-csv-data-in-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 101/1000 [00:19<02:44,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss 5506.34997901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 201/1000 [00:38<02:32,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss 5074.03569995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 301/1000 [00:57<02:05,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss 5381.88521721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 401/1000 [01:15<01:47,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss 4562.68957361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 500/1000 [01:33<01:33,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss 6073.50741278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 600/1000 [01:51<01:15,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss 6725.86122861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 701/1000 [02:10<00:53,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss 5206.01861421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 801/1000 [02:29<00:36,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss 5592.40694816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 901/1000 [02:47<00:17,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss 5966.19293244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 954/1000 [02:57<00:08,  5.59it/s]"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "for f in glob.glob(\"/tmp/model.ckpt*\"):\n",
    "    os.remove(f)\n",
    "\n",
    "saver = tf.train.Saver([w,b])\n",
    "EPOCHS = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Step 7: initialize the necessary variables, in this case, w and b\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Step 8: train the model\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    n_batches = int(n_train_data/BATCH_SIZE)\n",
    "    for epoch in tqdm(range(EPOCHS)): # run epochs\n",
    "        avg_loss = 0\n",
    "        \n",
    "        for _ in range(n_batches):\n",
    "            x_batch, y_batch = sess.run([data1_feature_batch, data1_label_batch])\n",
    "            # Session runs train_op to minimize loss\n",
    "            feed_dict={X: x_batch, Y:y_batch}\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            avg_loss += loss_batch/n_batches\n",
    "           \n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print \"avg_loss\",avg_loss\n",
    "            \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "    # Step 9: saving the values of w and b\n",
    "    print \"weights\",w.eval()\n",
    "    print \"bias\",b.eval()\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    save_path = saver.save(sess, \"/tmp/logit_reg_tf_model.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/logit_reg_tf_model.ckpt\n",
      "weights [[-1.82444668  1.82444668]\n",
      " [-0.19017248  0.19017248]\n",
      " [-0.12686753  0.12686753]\n",
      " [-1.23910809  1.23910809]\n",
      " [-0.1042711   0.1042711 ]\n",
      " [-0.84696817  0.84696817]\n",
      " [-0.5913415   0.5913415 ]\n",
      " [-5.48943043  5.48943043]\n",
      " [-3.7758956   3.7758956 ]]\n",
      "bias [[ 396.80010986 -396.80010986]]\n",
      "Accuracy 0.115646261342\n"
     ]
    }
   ],
   "source": [
    "# Step 10: predict\n",
    "# test the model\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"/tmp/logit_reg_tf_model.ckpt.meta\")\n",
    "with tf.Session() as sess:\n",
    "    # nitialize the necessary variables, in this case, w and b\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver.restore(sess, \"/tmp/logit_reg_tf_model.ckpt\")\n",
    "    print \"weights\",w.eval()\n",
    "    print \"bias\",b.eval()\n",
    "\n",
    "    total_correct_preds = 0\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    try:\n",
    "        for i in range(20):\n",
    "            x_batch, y_batch = sess.run([test_data1_feature_batch, test_data1_label_batch])\n",
    "            total_correct_preds += sess.run(accuracy, feed_dict={X: x_batch, Y:y_batch})\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done testing ...')\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "    print 'Accuracy {0}'.format(total_correct_preds/n_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
