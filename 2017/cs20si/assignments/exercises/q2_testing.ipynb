{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.client import device_lib\n",
    "print(tf.__version__)\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_epochs_completed', '_images', '_index_in_epoch', '_labels', '_num_examples', 'epochs_completed', 'images', 'labels', 'next_batch', 'num_examples']\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(5000, 784)\n",
      "(5000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "Tensor(\"Const:0\", shape=(784,), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(28, 28), dtype=float32)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxA+hxA+hxA+hxA+hxA+hxA+hxA+h/gv6b0fC3iB/UQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e5df18b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Step 1: read in data from the .xls file\n",
    "mnist = input_data.read_data_sets(\"../../data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "_, input_dim = mnist.train.images.shape\n",
    "_, output_dim = mnist.train.labels.shape\n",
    "\n",
    "print(dir(mnist.train))\n",
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)\n",
    "print(mnist.validation.images.shape)\n",
    "print(mnist.validation.labels.shape)\n",
    "print(mnist.test.images.shape)\n",
    "print(mnist.test.labels.shape)\n",
    "\n",
    "n_train, _ = mnist.train.images.shape\n",
    "some_index = np.random.randint(1,n_train)\n",
    "some_digit = tf.constant(mnist.train.images[some_index])\n",
    "print(some_digit)\n",
    "some_digit_image = tf.reshape(some_digit, [28,28])\n",
    "print(some_digit_image)\n",
    "\n",
    "print((mnist.train.labels[some_index]))\n",
    "plt.imshow(some_digit_image.eval())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/notMNIST-to-MNIST/data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/notMNIST-to-MNIST/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/notMNIST-to-MNIST/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/notMNIST-to-MNIST/data/t10k-labels-idx1-ubyte.gz\n",
      "['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_epochs_completed', '_images', '_index_in_epoch', '_labels', '_num_examples', 'epochs_completed', 'images', 'labels', 'next_batch', 'num_examples']\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(5000, 784)\n",
      "(5000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "Tensor(\"Const_1:0\", shape=(784,), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(28, 28), dtype=float32)\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABo5JREFUeJzt3X+o3XUdx/Fzjvdu88rd2Cb+2GZsTRaiqFPIzH/CWFOi\nsD8Ws0SUnGQI0f5o6vaHf6T9ESaaiBj9ESoIwlyjGLFSqJS5iSKYijlbpqnRvFds3rl1z/GvsH++\n73Pb8W6X83o8/n3te87553k/f3x27m33er0WkKdzoj8AcGKIH0KJH0KJH0KJH0KJH0KJH0KJH0KJ\nH0KNHM83W9fZ4L8Twizb3X2sPZN/5+SHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKH\nUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKHUOKH\nUOKHUOKHUOKHUOKHUOKHUOKHUOKHUCMn+gMwt42ceUa5H7rwrHKfv2vfMb93e+255f6XzfPK/ceX\nbG/cfnbLxvLZse3PlPswcPJDKPFDKPFDKPFDKPFDKPFDKPFDKPf8lF773qpy33v9T8t93bbNjdvU\nae3y2V/cdE+5r5137GfXlq/0yn1N838RGBpOfgglfgglfgglfgglfgglfgjlqi/dF84v5/VXPlvu\nY+36a7VP3XHf//2RPlGfTf/uflTuH/S6jduK39bXjAmc/BBK/BBK/BBK/BBK/BBK/BBK/BDKPf+Q\n64yPl/tlD+4t91uXvlTuzTfp/T11eLTcb9hxY7mv2lHf83f++HzjNtYa/l/N3Y+TH0KJH0KJH0KJ\nH0KJH0KJH0KJH0K55x9ynaWLy/3qRbv6vMLYQO+/5tffbdzOuW1/+ezqg3sGem9qTn4IJX4IJX4I\nJX4IJX4IJX4IJX4I5Z5/yP3nwBvlvvXNr5f7Iyt/V+6PH1pS7udsebVxm558v3yW2eXkh1Dih1Di\nh1Dih1Dih1Dih1Dih1Du+YfcSQsXlvvahX8f6PUffvvScp+efGeg12f2OPkhlPghlPghlPghlPgh\nlPghlKu+IffuxnPLffOSJ/u8Qrtc//qbz5b7sparvrnKyQ+hxA+hxA+hxA+hxA+hxA+hxA+h3PMP\nu68dHOjx7/+j/sru8rufLffeQO/ObHLyQyjxQyjxQyjxQyjxQyjxQyjxQyj3/ENg8trmu/jHL/hJ\nn6dPLtd/HTml3HtH3+vz+sxVTn4IJX4IJX4IJX4IJX4IJX4IJX4I5Z5/CBxcf7hxW37S2ECv/eiq\nJ8p9+q1uuU/1jjRun99zQ/nsyk1v1e89MVHu1Jz8EEr8EEr8EEr8EEr8EEr8EEr8EMo9/xDo/XN+\n4zbRnSqfXdRZ0OfF63v8bp/fzD+/Pdq4vXDpL8tnr1rw1XJnME5+CCV+CCV+CCV+CCV+CCV+COWq\nbwic/YM9jdvVO24un339quZrwlar1Xr1m/cf02eaiUc+OLPce1P1NSWDcfJDKPFDKPFDKPFDKPFD\nKPFDKPFDKPf8Q27kudfK/TPblszq+1//ty83bu9dt7R8dnpy/6f9cfgfTn4IJX4IJX4IJX4IJX4I\nJX4IJX4I5Z5/CEx/6aLGbfEdB8pnd67c3ufV2+X6ud9vKvc133mxcesdnezz3swmJz+EEj+EEj+E\nEj+EEj+EEj+EEj+Ecs8/B3TGx8v99VvOK/eHvnVv47Z2Xv3zfaJ7uNy/+If69/6v2fTncu8dPVLu\nnDhOfgglfgglfgglfgglfgglfgjlqm+G2qPzGrep9ReWz56y70C5L995qNx3rriv3Kuf4W9P13/m\n+soHfljuq+98utx75cpc5uSHUOKHUOKHUOKHUOKHUOKHUOKHUO75Z2j/jy5u3J7/9t3lsx/2pst9\ncWfBMX2m/7ri5W80btN3nV4+u2JXfY/P8HLyQyjxQyjxQyjxQyjxQyjxQyjxQyj3/DO0/vLnGrf5\n7dHy2X77zkOLy/32n19T7svueqZxG+m+UT5LLic/hBI/hBI/hBI/hBI/hBI/hBI/hHLPP0N/eqj5\n+/ytLfV34i/bWv+Z61N/9Uq5L5vwnXs+fU5+CCV+CCV+CCV+CCV+CCV+CCV+CNXu9Y7fX1hf19ng\nz7nDLNvdfaw9k3/n5IdQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ\n4odQ4odQ4odQ4odQ4odQ4odQ4odQx/VXdwNzh5MfQokfQokfQokfQokfQokfQokfQokfQokfQokf\nQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQokfQn0Mg3DQUUwflFEAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e505d8990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "not_mnist = input_data.read_data_sets(\"../../data/notMNIST-to-MNIST/data\", one_hot=True)\n",
    "\n",
    "_, input_dim = not_mnist.train.images.shape\n",
    "_, output_dim = not_mnist.train.labels.shape\n",
    "\n",
    "print(dir(not_mnist.train))\n",
    "print(not_mnist.train.images.shape)\n",
    "print(not_mnist.train.labels.shape)\n",
    "print(not_mnist.validation.images.shape)\n",
    "print(not_mnist.validation.labels.shape)\n",
    "print(not_mnist.test.images.shape)\n",
    "print(not_mnist.test.labels.shape)\n",
    "\n",
    "n_train, _ = not_mnist.train.images.shape\n",
    "some_index = np.random.randint(1,n_train)\n",
    "some_alpha = tf.constant(not_mnist.train.images[some_index])\n",
    "print(some_alpha)\n",
    "some_alpha_image = tf.reshape(some_alpha, [28,28])\n",
    "print(some_alpha_image)\n",
    "\n",
    "print((not_mnist.train.labels[some_index]))\n",
    "plt.imshow(some_alpha_image.eval())\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: create placeholders for input X (Image) and label Y (number)\n",
    "X = tf.placeholder(tf.float32, shape=[None, input_dim], name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, shape=[None,output_dim], name=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 1000\n",
    "learning_rate=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X:0\", shape=(?, 784), dtype=float32)\n",
      "Tensor(\"Y:0\", shape=(?, 10), dtype=float32)\n",
      "<tf.Variable 'weights:0' shape=(784, 10) dtype=float32_ref>\n",
      "<tf.Variable 'bias:0' shape=(10,) dtype=float32_ref>\n",
      "Tensor(\"add:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"Reshape_4:0\", shape=(?,), dtype=float32)\n",
      "name: \"GradientDescent\"\n",
      "op: \"NoOp\"\n",
      "input: \"^GradientDescent/update_weights/ApplyGradientDescent\"\n",
      "input: \"^GradientDescent/update_bias/ApplyGradientDescent\"\n",
      "\n",
      "Tensor(\"Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"Equal:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: create weight and bias, initialized to 0\n",
    "w = tf.Variable(tf.truncated_normal([input_dim, output_dim]), name=\"weights\")\n",
    "b = tf.Variable(tf.truncated_normal([output_dim]), name=\"bias\")\n",
    "\n",
    "# Step 4: logistic multinomial regression / softmax\n",
    "score = tf.matmul(X, w) + b\n",
    "\n",
    "# Step 5: define loss function\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=score, labels=Y, name=\"loss\")\n",
    "\n",
    "# Step 6: using gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Step 7: Prediction\n",
    "Y_predicted = tf.nn.softmax(tf.matmul(X, w) + b)\n",
    "correct_prediction = tf.equal(tf.argmax(Y,1), tf.argmax(Y_predicted,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(X)\n",
    "print(Y)\n",
    "print(w)\n",
    "print(b)\n",
    "print(score)\n",
    "print(loss)\n",
    "print(optimizer)\n",
    "print(Y_predicted)\n",
    "print(correct_prediction)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 826.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8686\n",
      "0.8601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    # Step 7: initialize the necessary variables, in this case, w and b\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Step 8: train the model\n",
    "    for i in tqdm(range(epochs)): # run epochs\n",
    "        x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        # Session runs train_op to minimize loss\n",
    "#         l = sess.run(loss)\n",
    "#         print(np.shape(loss))\n",
    "        sess.run(optimizer, feed_dict={X: x_batch, Y:y_batch})\n",
    "\n",
    "    # Step 9: output the values of w and b\n",
    "    w_value, b_value = sess.run([w, b])\n",
    "\n",
    "    # Step 10: predict\n",
    "    print(sess.run(accuracy, feed_dict={X: mnist.validation.images, Y:mnist.validation.labels}))\n",
    "    print(sess.run(accuracy, feed_dict={X: mnist.test.images, Y:mnist.test.labels}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, name=None):\n",
    "  return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=name)\n",
    "\n",
    "def bias_variable(shape, name=None):\n",
    "  return tf.Variable(tf.constant(0.1, shape=shape), name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "epochs = 5000 #20000\n",
    "learning_rate=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=y_conv))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(Y, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in tqdm(range(epochs)):\n",
    "        x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={X: x_batch, Y: y_batch, keep_prob: 1.0})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "\n",
    "        train_step.run(feed_dict={X: x_batch, Y: y_batch, keep_prob: 0.5})\n",
    "    \n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={X: x_batch, Y: y_batch, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 462 entries, 0 to 461\n",
      "Data columns (total 10 columns):\n",
      "sbp          462 non-null int64\n",
      "tobacco      462 non-null float64\n",
      "ldl          462 non-null float64\n",
      "adiposity    462 non-null float64\n",
      "famhist      462 non-null object\n",
      "typea        462 non-null int64\n",
      "obesity      462 non-null float64\n",
      "alcohol      462 non-null float64\n",
      "age          462 non-null int64\n",
      "chd          462 non-null int64\n",
      "dtypes: float64(5), int64(4), object(1)\n",
      "memory usage: 36.2+ KB\n",
      "None\n",
      "['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age', 'chd']\n",
      "   sbp  tobacco   ldl  adiposity  famhist  typea  obesity  alcohol  age  chd\n",
      "0  160    12.00  5.73      23.11  Present     49    25.30    97.20   52    1\n",
      "1  144     0.01  4.41      28.61   Absent     55    28.87     2.06   63    1\n",
      "2  118     0.08  3.48      32.28  Present     52    29.14     3.81   46    0\n",
      "3  170     7.50  6.41      38.03  Present     51    31.99    24.26   58    1\n",
      "4  134    13.60  3.50      27.78  Present     60    25.99    57.34   49    1\n"
     ]
    }
   ],
   "source": [
    "heart_data = pd.read_csv(\"../../data/heart.csv\")\n",
    "print heart_data.info()\n",
    "print [name for name in heart_data.columns]\n",
    "print heart_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Lines from CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample code reading per csv line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([160, 12.0, 5.73, 23.110001, 'Present', 49, 25.299999, 97.199997, 52], 1)\n",
      "([144, 0.0099999998, 4.4099998, 28.610001, 'Absent', 55, 28.870001, 2.0599999, 63], 1)\n",
      "([118, 0.079999998, 3.48, 32.279999, 'Present', 52, 29.139999, 3.8099999, 46], 0)\n",
      "([170, 7.5, 6.4099998, 38.029999, 'Present', 51, 31.99, 24.26, 58], 1)\n",
      "([134, 13.6, 3.5, 27.780001, 'Present', 60, 25.99, 57.34, 49], 1)\n",
      "([132, 6.1999998, 6.4699998, 36.209999, 'Present', 62, 30.77, 14.14, 45], 0)\n",
      "([142, 4.0500002, 3.3800001, 16.200001, 'Absent', 59, 20.809999, 2.6199999, 38], 0)\n",
      "([114, 4.0799999, 4.5900002, 14.6, 'Present', 62, 23.110001, 6.7199998, 58], 1)\n",
      "([114, 0.0, 3.8299999, 19.4, 'Present', 49, 24.860001, 2.49, 29], 0)\n",
      "([132, 0.0, 5.8000002, 30.959999, 'Present', 69, 30.110001, 0.0, 53], 1)\n"
     ]
    }
   ],
   "source": [
    "filename_queue = tf.train.string_input_producer([\"../../data/heart.csv\"])\n",
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "_, csv_row = reader.read(filename_queue)\n",
    "\n",
    "record_defaults = [[0], [0.0], [0.0], [0.0], [\"\"], [0], [0.0], [0.0], [0], [0]]\n",
    "sbp, tobacco, ldl, adiposity, famhist, typea, obesity, alcohol, age, chd = tf.decode_csv(csv_row, record_defaults=record_defaults)\n",
    "features = [sbp, tobacco, ldl, adiposity, famhist, typea, obesity, alcohol, age]\n",
    "\n",
    "nof_examples = 10\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    while nof_examples > 0:\n",
    "        nof_examples -= 1\n",
    "        try:\n",
    "            data_features, data_chd = sess.run([features, chd])\n",
    "#             data_features[4] = 1 if data_features[4] == 'Present' else 0\n",
    "            print(data_features, data_chd)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            break\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample reading batch of lines from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'IteratorGetNext_2:0' shape=(?,) dtype=int32>, <tf.Tensor 'IteratorGetNext_2:1' shape=(?,) dtype=float32>, <tf.Tensor 'IteratorGetNext_2:2' shape=(?,) dtype=float32>, <tf.Tensor 'IteratorGetNext_2:3' shape=(?,) dtype=float32>, <tf.Tensor 'IteratorGetNext_2:4' shape=(?,) dtype=int32>, <tf.Tensor 'IteratorGetNext_2:5' shape=(?,) dtype=int32>, <tf.Tensor 'IteratorGetNext_2:6' shape=(?,) dtype=float32>, <tf.Tensor 'IteratorGetNext_2:7' shape=(?,) dtype=float32>, <tf.Tensor 'IteratorGetNext_2:8' shape=(?,) dtype=int32>]\n",
      "Tensor(\"IteratorGetNext_2:9\", shape=(?,), dtype=int32)\n",
      "==================================\n",
      "('data_features', [array([114, 132, 160, 132, 170], dtype=int32), array([  4.07999992,   0.        ,  12.        ,   6.19999981,   7.5       ], dtype=float32), array([ 4.59000015,  5.80000019,  5.73000002,  6.46999979,  6.40999985], dtype=float32), array([ 14.60000038,  30.95999908,  23.11000061,  36.20999908,  38.02999878], dtype=float32), array([1, 1, 1, 1, 1], dtype=int32), array([62, 69, 49, 62, 51], dtype=int32), array([ 23.11000061,  30.11000061,  25.29999924,  30.77000046,  31.98999977], dtype=float32), array([  6.71999979,   0.        ,  97.19999695,  14.14000034,  24.26000023], dtype=float32), array([58, 53, 52, 45, 58], dtype=int32)])\n",
      "('data_labels', array([1, 1, 1, 0, 1], dtype=int32))\n",
      "==================================\n",
      "('data_features', [array([112, 142, 134, 118, 117], dtype=int32), array([  9.64999962,   4.05000019,  14.10000038,   0.08      ,   1.52999997], dtype=float32), array([ 2.28999996,  3.38000011,  4.44000006,  3.48000002,  2.44000006], dtype=float32), array([ 17.20000076,  16.20000076,  22.38999939,  32.27999878,  28.95000076], dtype=float32), array([1, 0, 1, 1, 1], dtype=int32), array([54, 59, 65, 52, 35], dtype=int32), array([ 23.53000069,  20.80999947,  23.09000015,  29.13999939,  25.88999939], dtype=float32), array([  0.68000001,   2.61999989,   0.        ,   3.80999994,  30.03000069], dtype=float32), array([53, 38, 40, 46, 46], dtype=int32)])\n",
      "('data_labels', array([0, 0, 1, 0, 0], dtype=int32))\n",
      "==================================\n",
      "('data_features', [array([146, 114, 118, 124, 158], dtype=int32), array([ 10.5      ,   0.       ,   0.       ,  14.       ,   2.5999999], dtype=float32), array([ 8.28999996,  3.82999992,  1.88      ,  6.23000002,  7.46000004], dtype=float32), array([ 35.36000061,  19.39999962,  10.05000019,  35.95999908,  34.06999969], dtype=float32), array([1, 1, 0, 1, 1], dtype=int32), array([78, 49, 59, 45, 61], dtype=int32), array([ 32.72999954,  24.86000061,  21.56999969,  30.09000015,  29.29999924], dtype=float32), array([ 13.89000034,   2.49000001,   0.        ,   0.        ,  53.27999878], dtype=float32), array([53, 29, 17, 59, 62], dtype=int32)])\n",
      "('data_labels', array([1, 0, 0, 1, 1], dtype=int32))\n",
      "==================================\n",
      "('data_features', [array([144, 124, 206, 120, 144], dtype=int32), array([ 0.01      ,  4.        ,  6.        ,  7.5       ,  4.09000015], dtype=float32), array([  4.40999985,  12.42000008,   2.95000005,  15.32999992,   5.55000019], dtype=float32), array([ 28.61000061,  31.29000092,  32.27000046,  22.        ,  31.39999962], dtype=float32), array([0, 1, 0, 0, 1], dtype=int32), array([55, 54, 72, 60, 60], dtype=int32), array([ 28.87000084,  23.22999954,  26.80999947,  25.30999947,  29.43000031], dtype=float32), array([  2.05999994,   2.05999994,  56.06000137,  34.49000168,   5.55000019], dtype=float32), array([63, 42, 60, 49, 56], dtype=int32)])\n",
      "('data_labels', array([1, 1, 1, 0, 0], dtype=int32))\n",
      "==================================\n",
      "('data_features', [array([132, 150, 158, 146, 138], dtype=int32), array([ 7.9000001 ,  0.30000001,  1.01999998,  0.        ,  0.60000002], dtype=float32), array([ 2.8499999 ,  6.38000011,  6.32999992,  6.61999989,  3.80999994], dtype=float32), array([ 26.5       ,  33.99000168,  23.87999916,  25.69000053,  28.65999985], dtype=float32), array([1, 1, 0, 0, 0], dtype=int32), array([51, 62, 66, 60, 54], dtype=int32), array([ 26.15999985,  24.63999939,  22.12999916,  28.06999969,  28.70000076], dtype=float32), array([ 25.70999908,   0.        ,  24.98999977,   8.22999954,   1.46000004], dtype=float32), array([44, 50, 46, 63, 58], dtype=int32)])\n",
      "('data_labels', array([0, 0, 1, 1, 0], dtype=int32))\n",
      "==================================\n",
      "('data_features', [array([145, 118, 148, 110, 106], dtype=int32), array([ 9.10000038,  6.        ,  5.5       ,  4.63999987,  1.61000001], dtype=float32), array([ 5.23999977,  9.64999962,  7.0999999 ,  4.55000019,  1.74000001], dtype=float32), array([ 27.54999924,  33.90999985,  25.30999947,  30.45999908,  12.31999969], dtype=float32), array([0, 0, 0, 0, 0], dtype=int32), array([59, 60, 56, 48, 74], dtype=int32), array([ 20.95999908,  38.79999924,  29.84000015,  30.89999962,  20.92000008], dtype=float32), array([ 21.60000038,   0.        ,   3.5999999 ,  15.22000027,  13.36999989], dtype=float32), array([61, 48, 48, 46, 20], dtype=int32)])\n",
      "('data_labels', array([1, 0, 0, 0, 1], dtype=int32))\n",
      "==================================\n",
      "('data_features', [array([130, 132, 126, 144, 130], dtype=int32), array([ 0.       ,  0.       ,  8.75     ,  0.04     ,  2.6099999], dtype=float32), array([ 2.81999993,  1.87      ,  6.53000021,  3.38000011,  2.72000003], dtype=float32), array([ 19.62999916,  17.20999908,  34.02000046,  23.61000061,  22.98999977], dtype=float32), array([1, 0, 0, 0, 1], dtype=int32), array([70, 49, 49, 30, 51], dtype=int32), array([ 24.86000061,  23.62999916,  30.25      ,  23.75      ,  26.29000092], dtype=float32), array([  0.        ,   0.97000003,   0.        ,   4.65999985,  13.36999989], dtype=float32), array([29, 15, 41, 30, 51], dtype=int32)])\n",
      "('data_labels', array([0, 0, 1, 0, 1], dtype=int32))\n",
      "==================================\n",
      "('data_features', [array([120, 136, 142, 134, 116], dtype=int32), array([  0.        ,  11.19999981,  18.20000076,  13.60000038,   1.90999997], dtype=float32), array([ 1.07000005,  5.80999994,  4.34000015,  3.5       ,  7.55999994], dtype=float32), array([ 16.02000046,  31.85000038,  24.37999916,  27.78000069,  26.45000076], dtype=float32), array([0, 1, 0, 1, 1], dtype=int32), array([47, 75, 61, 60, 52], dtype=int32), array([ 22.14999962,  27.68000031,  26.19000053,  25.98999977,  30.01000023], dtype=float32), array([  0.        ,  22.94000053,   0.        ,  57.34000015,   3.5999999 ], dtype=float32), array([15, 58, 50, 49, 33], dtype=int32)])\n",
      "('data_labels', array([0, 1, 0, 1, 1], dtype=int32))\n",
      "==================================\n",
      "('data_features', [array([162, 126, 136, 122, 122], dtype=int32), array([ 7.4000001 ,  3.79999995,  2.51999998,  6.5999999 ,  0.        ], dtype=float32), array([ 8.55000019,  3.88000011,  3.95000005,  5.57999992,  5.75      ], dtype=float32), array([ 24.64999962,  31.79000092,  25.62999916,  35.95000076,  30.89999962], dtype=float32), array([1, 0, 0, 1, 1], dtype=int32), array([64, 57, 51, 53, 46], dtype=int32), array([ 25.70999908,  30.53000069,  21.86000061,  28.06999969,  29.01000023], dtype=float32), array([  5.86000013,   0.        ,   0.        ,  12.55000019,   4.11000013], dtype=float32), array([58, 30, 45, 59, 42], dtype=int32)])\n",
      "('data_labels', array([1, 0, 1, 1, 0], dtype=int32))\n",
      "==================================\n",
      "('data_features', [array([134, 114, 128, 140, 134], dtype=int32), array([ 2.5       ,  0.        ,  4.6500001 ,  3.9000001 ,  8.07999992], dtype=float32), array([ 3.66000009,  1.94000006,  3.30999994,  7.32000017,  1.54999995], dtype=float32), array([ 30.89999962,  11.02000046,  22.73999977,  25.04999924,  17.5       ], dtype=float32), array([0, 0, 0, 0, 1], dtype=int32), array([52, 54, 62, 47, 56], dtype=int32), array([ 27.19000053,  20.17000008,  22.95000076,  27.36000061,  22.64999962], dtype=float32), array([ 23.65999985,  38.97999954,   0.50999999,  36.77000046,  66.65000153], dtype=float32), array([49, 16, 48, 32, 31], dtype=int32)])\n",
      "('data_labels', array([0, 0, 0, 0, 1], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "def read_row(csv_row):\n",
    "    record_defaults = [[0], [0.0], [0.0], [0.0], [\"\"], [0], [0.0], [0.0], [0], [0]]\n",
    "    row = tf.decode_csv(csv_row, record_defaults=record_defaults)\n",
    "    row[4] = tf.cond(tf.equal(row[4], \"Present\"), lambda: 1, lambda: 0)\n",
    "    return row[:-1], row[-1]\n",
    "\n",
    "def input_pipeline(filenames, batch_size):\n",
    "    # Define a `tf.contrib.data.Dataset` for iterating over one epoch of the data.\n",
    "    dataset = (tf.contrib.data.TextLineDataset(filenames)\n",
    "               .skip(1)\n",
    "               .map(lambda line: read_row(line))\n",
    "               .shuffle(buffer_size=10)  # Equivalent to min_after_dequeue=10.\n",
    "               .batch(batch_size))\n",
    "\n",
    "    # Return an *initializable* iterator over the dataset, which will allow us to\n",
    "    # re-initialize it at the beginning of each epoch.\n",
    "    return dataset.make_initializable_iterator()\n",
    "\n",
    "batch_size = 5\n",
    "data_iterator = input_pipeline(['../../data/heart.csv'], batch_size)\n",
    "features, labels = data_iterator.get_next()\n",
    "print(features)\n",
    "print(labels)\n",
    "\n",
    "nof_examples = 10\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    sess.run(data_iterator.initializer)\n",
    "    while nof_examples > 0:\n",
    "        nof_examples -= 1\n",
    "        try:\n",
    "            data_features, data_labels = sess.run([features, labels])\n",
    "\n",
    "            print(\"==================================\")\n",
    "            print(\"data_features\",data_features)\n",
    "            print(\"data_labels\",data_labels)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 112.            9.64999962    2.28999996   17.20000076    1.           54.\n",
      "    23.53000069    0.68000001   53.        ]\n",
      " [ 106.            1.61000001    1.74000001   12.31999969    0.           74.\n",
      "    20.92000008   13.36999989   20.        ]\n",
      " [ 134.           14.10000038    4.44000006   22.38999939    1.           65.\n",
      "    23.09000015    0.           40.        ]]\n",
      "[[ 117.            1.52999997    2.44000006   28.95000076    1.           35.\n",
      "    25.88999939   30.03000069   46.        ]\n",
      " [ 126.            8.75          6.53000021   34.02000046    0.           49.\n",
      "    30.25          0.           41.        ]\n",
      " [ 206.            6.            2.95000005   32.27000046    0.           72.\n",
      "    26.80999947   56.06000137   60.        ]]\n",
      "[[ 122.            4.26000023    4.44000006   13.03999996    0.           57.\n",
      "    19.48999977   48.99000168   28.        ]\n",
      " [ 110.            4.63999987    4.55000019   30.45999908    0.           48.\n",
      "    30.89999962   15.22000027   46.        ]\n",
      " [ 124.            4.           12.42000008   31.29000092    1.           54.\n",
      "    23.22999954    2.05999994   42.        ]]\n",
      "[[ 142.           18.20000076    4.34000015   24.37999916    0.           61.\n",
      "    26.19000053    0.           50.        ]\n",
      " [ 118.            0.            1.88         10.05000019    0.           59.\n",
      "    21.56999969    0.           17.        ]\n",
      " [ 136.           11.19999981    5.80999994   31.85000038    1.           75.\n",
      "    27.68000031   22.94000053   58.        ]]\n",
      "[[  1.18000000e+02   7.99999982e-02   3.48000002e+00   3.22799988e+01\n",
      "    1.00000000e+00   5.20000000e+01   2.91399994e+01   3.80999994e+00\n",
      "    4.60000000e+01]\n",
      " [  1.44000000e+02   3.99999991e-02   3.38000011e+00   2.36100006e+01\n",
      "    0.00000000e+00   3.00000000e+01   2.37500000e+01   4.65999985e+00\n",
      "    3.00000000e+01]\n",
      " [  1.14000000e+02   4.07999992e+00   4.59000015e+00   1.46000004e+01\n",
      "    1.00000000e+00   6.20000000e+01   2.31100006e+01   6.71999979e+00\n",
      "    5.80000000e+01]]\n",
      "[[ 124.           14.            6.23000002   35.95999908    1.           45.\n",
      "    30.09000015    0.           59.        ]\n",
      " [ 170.            7.5           6.40999985   38.02999878    1.           51.\n",
      "    31.98999977   24.26000023   58.        ]\n",
      " [ 118.            0.28          5.80000019   33.70000076    1.           60.\n",
      "    30.97999954    0.           41.        ]]\n",
      "[[ 120.            0.            1.07000005   16.02000046    0.           47.\n",
      "    22.14999962    0.           15.        ]\n",
      " [ 122.            6.5999999     5.57999992   35.95000076    1.           53.\n",
      "    28.06999969   12.55000019   59.        ]\n",
      " [ 114.            0.            3.82999992   19.39999962    1.           49.\n",
      "    24.86000061    2.49000001   29.        ]]\n",
      "[[ 152.            0.89999998    9.11999989   30.22999954    0.           56.\n",
      "    28.63999939    0.37         42.        ]\n",
      " [ 132.            6.19999981    6.46999979   36.20999908    1.           62.\n",
      "    30.77000046   14.14000034   45.        ]\n",
      " [ 130.            2.6099999     2.72000003   22.98999977    1.           51.\n",
      "    26.29000092   13.36999989   51.        ]]\n",
      "[[ 138.            0.60000002    3.80999994   28.65999985    0.           54.\n",
      "    28.70000076    1.46000004   58.        ]\n",
      " [ 156.            3.            1.82000005   27.54999924    0.           60.\n",
      "    23.90999985   54.           53.        ]\n",
      " [ 132.            0.            1.87         17.20999908    0.           49.\n",
      "    23.62999916    0.97000003   15.        ]]\n",
      "[[ 146.           10.5           8.28999996   35.36000061    1.           78.\n",
      "    32.72999954   13.89000034   53.        ]\n",
      " [ 158.            2.5999999     7.46000004   34.06999969    1.           61.\n",
      "    29.29999924   53.27999878   62.        ]\n",
      " [ 160.           12.            5.73000002   23.11000061    1.           49.\n",
      "    25.29999924   97.19999695   52.        ]]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '../../data/heart.csv'\n",
    "BATCH_SIZE = 3\n",
    "N_FEATURES = 9\n",
    "\n",
    "def batch_generator(filenames):\n",
    "    \"\"\" filenames is the list of files you want to read from. \n",
    "    In this case, it contains only heart.csv\n",
    "    \"\"\"\n",
    "    filename_queue = tf.train.string_input_producer(filenames)\n",
    "    reader = tf.TextLineReader(skip_header_lines=1) # skip the first line in the file\n",
    "    _, value = reader.read(filename_queue)\n",
    "\n",
    "    # record_defaults are the default values in case some of our columns are empty\n",
    "    # This is also to tell tensorflow the format of our data (the type of the decode result)\n",
    "    # for this dataset, out of 9 feature columns, \n",
    "    # 8 of them are floats (some are integers, but to make our features homogenous, \n",
    "    # we consider them floats), and 1 is string (at position 5)\n",
    "    # the last column corresponds to the lable is an integer\n",
    "\n",
    "    record_defaults = [[1.0] for _ in range(N_FEATURES)]\n",
    "    record_defaults[4] = ['']\n",
    "    record_defaults.append([1])\n",
    "\n",
    "    # read in the 10 rows of data\n",
    "    content = tf.decode_csv(value, record_defaults=record_defaults) \n",
    "\n",
    "    # convert the 5th column (present/absent) to the binary value 0 and 1\n",
    "    condition = tf.equal(content[4], tf.constant('Present'))\n",
    "    content[4] = tf.cond(condition, lambda: tf.constant(1.0), lambda: tf.constant(0.0))\n",
    "\n",
    "    # pack all 9 features into a tensor\n",
    "    features = tf.stack(content[:N_FEATURES])\n",
    "\n",
    "    # assign the last column to label\n",
    "    label = content[-1]\n",
    "\n",
    "    # minimum number elements in the queue after a dequeue, used to ensure \n",
    "    # that the samples are sufficiently mixed\n",
    "    # I think 10 times the BATCH_SIZE is sufficient\n",
    "    min_after_dequeue = 10 * BATCH_SIZE\n",
    "\n",
    "    # the maximum number of elements in the queue\n",
    "    capacity = 20 * BATCH_SIZE\n",
    "\n",
    "    # shuffle the data to generate BATCH_SIZE sample pairs\n",
    "    data_batch, label_batch = tf.train.shuffle_batch([features, label], batch_size=BATCH_SIZE, \n",
    "                                        capacity=capacity, min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "    return data_batch, label_batch\n",
    "\n",
    "def generate_batches(data_batch, label_batch):\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        for _ in range(10): # generate 10 batches\n",
    "            features, labels = sess.run([data_batch, label_batch])\n",
    "            print features\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "data_batch, label_batch = batch_generator([DATA_PATH])\n",
    "generate_batches(data_batch, label_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
