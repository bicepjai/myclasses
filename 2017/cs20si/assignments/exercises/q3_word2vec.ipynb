{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'/gpu:0', u'/gpu:1']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(tf.__version__)\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b733bb67315b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# os.remove(\"./vocab_1000.tsv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# os.remove(\"/tmp/tb/svd/vocab_1000.tsv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mindex_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_words_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bicepjai/Projects/myclasses/2017/cs20si/assignments/exercises/process_data.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(words, vocab_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UNK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vocab_1000.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0mself_get\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from process_data import read_data, build_vocab, convert_words_to_index\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "words = read_data(\"../../data/text8.zip\")\n",
    "\n",
    "# os.remove(\"./vocab_1000.tsv\")\n",
    "# os.remove(\"/tmp/tb/svd/vocab_1000.tsv\")\n",
    "dictionary, _ = build_vocab(words, VOCAB_SIZE)\n",
    "index_words = convert_words_to_index(words, dictionary)\n",
    "print len(words)\n",
    "\n",
    "shutil.copyfile(\"./vocab_1000.tsv\",\"/tmp/tb/svd/vocab_1000.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cooccurrence_matrix = np.zeros((VOCAB_SIZE, VOCAB_SIZE))\n",
    "n_words = len(words)\n",
    "for i, current_word in enumerate(words):\n",
    "    if current_word not in dictionary:\n",
    "        current_word = 'UNK'\n",
    "\n",
    "    if i != 0:\n",
    "        left_word = words[i-1]\n",
    "        if left_word not in dictionary:\n",
    "            left_word = 'UNK'\n",
    "        cooccurrence_matrix[dictionary[current_word]][dictionary[left_word]] += 1\n",
    "\n",
    "    if i < n_words-1:\n",
    "        right_word = words[i+1]\n",
    "        if right_word not in dictionary:\n",
    "            right_word = 'UNK'\n",
    "        cooccurrence_matrix[dictionary[current_word]][dictionary[right_word]] += 1\n",
    "\n",
    "del words\n",
    "print cooccurrence_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tf_svd = tf.svd(cooccurrence_matrix, compute_uv=True)\n",
    "svd, final_embed_matrix, v = None, None, None\n",
    "\n",
    "with tf.device(\"/gpu:1\"):\n",
    "    with tf.Session() as sess:\n",
    "        start_time = time.time()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        svd, final_embed_matrix, v = sess.run(tf_svd)\n",
    "        print \"Total_time to run:\", time.time() - start_time\n",
    "\n",
    "        # code to visualize the embeddings\n",
    "        # it has to variable. constants don't work here. you can't reuse model.embed_matrix\n",
    "        embedding_var = tf.Variable(final_embed_matrix[:1000], name='embedding')\n",
    "        sess.run(embedding_var.initializer)\n",
    "\n",
    "        config = projector.ProjectorConfig()\n",
    "        summary_writer = tf.summary.FileWriter('/tmp/tb/svd')\n",
    "\n",
    "        # add embedding to the config file\n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = embedding_var.name\n",
    "\n",
    "        # link this tensor to its metadata file, in this case the first 500 words of vocab\n",
    "        embedding.metadata_path = '/tmp/tb/svd/vocab_1000.tsv'\n",
    "\n",
    "        # saves a configuration file that TensorBoard will read during startup.\n",
    "        projector.visualize_embeddings(summary_writer, config)\n",
    "        saver_embed = tf.train.Saver()\n",
    "        saver_embed.save(sess, '/tmp/tb/svd/svd.ckpt', 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os, shutil\n",
    "for f in glob.glob(\"/tmp/tb/word2vec/*\"):\n",
    "    shutil.rmtree(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://danijar.com/structuring-your-tensorflow-models/\n",
    "def doublewrap(function):\n",
    "    \"\"\"\n",
    "    A decorator decorator, allowing to use the decorator to be used without\n",
    "    parentheses if not arguments are provided. All arguments must be optional.\n",
    "    \"\"\"\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@doublewrap\n",
    "def define_scope(function, scope=None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    A decorator for functions that define TensorFlow operations. The wrapped\n",
    "    function will only be executed once. Subsequent calls to it will directly\n",
    "    return the result so that operations are added to the graph only once.\n",
    "    The operations added by the function live within a tf.variable_scope(). If\n",
    "    this decorator is used with arguments, they will be forwarded to the\n",
    "    variable scope. The scope name defaults to the name of the wrapped\n",
    "    function.\n",
    "    \"\"\"\n",
    "    attribute = '_cache_' + function.__name__\n",
    "    name = scope or function.__name__\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            with tf.variable_scope(name, *args, **kwargs):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "SKIP_WINDOW = 1 # the context window\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128 # dimension of the word embedding vectors\n",
    "N_NEG_SAMPLES = 32    # Number of negative examples to sample.\n",
    "LEARNING_RATE = 1.0\n",
    "EPOCHS = 10000\n",
    "SKIP_STEP = 2000 #how many steps to skip before reporting the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramModel:\n",
    "    \"\"\" Build the graph for word2vec model \"\"\"\n",
    "    def __init__(self, batch_generator, epochs, learning_rate):\n",
    "        \n",
    "        self.batch_generator = batch_generator\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.global_step = tf.Variable(0, trainable=False, dtype=tf.int32, name=\"global_step\")\n",
    "        \n",
    "        self.Placeholders\n",
    "        self.Embedding\n",
    "        self.Loss\n",
    "        self.Optimizer\n",
    "        self.Summaries\n",
    "\n",
    "    @define_scope\n",
    "    def Placeholders(self):\n",
    "        \"\"\" Step 1: define the placeholders for input and output \"\"\"\n",
    "        self.batch_inputs = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name=\"center_words\")\n",
    "        self.batch_labels = tf.placeholder(tf.int32, shape=[BATCH_SIZE,1], name=\"target_words\")\n",
    "    \n",
    "    @define_scope\n",
    "    def Embedding(self):\n",
    "        \"\"\" Step 2: define weights. In word2vec, it's actually the weights that we care about \"\"\"\n",
    "        embedding_matrix = tf.Variable(tf.random_uniform([BATCH_SIZE, EMBED_SIZE], -1.0, 1.0), \n",
    "                                            name=\"embedding_matrix\")\n",
    "        self.batch_embeddings = tf.nn.embedding_lookup(embedding_matrix, self.batch_inputs)\n",
    "        \n",
    "        norm = tf.sqrt(tf.reduce_mean(tf.square(self.batch_embeddings), 1, keep_dims=True))\n",
    "        self.normalized_embedding_matrix = self.batch_embeddings / norm\n",
    "        \n",
    "        \n",
    "    @define_scope\n",
    "    def Loss(self):\n",
    "        \"\"\" Step 3 + 4: define the inference + the loss function \"\"\"\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0/EMBED_SIZE ** 0.5))\n",
    "        nce_biases   = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "        self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights, biases=nce_biases, \n",
    "                                             labels=tf.cast(self.batch_labels, tf.float32),\n",
    "                                             inputs=self.batch_embeddings,\n",
    "                                             num_sampled=N_NEG_SAMPLES, num_classes=VOCAB_SIZE))\n",
    "        \n",
    "    @define_scope\n",
    "    def Optimizer(self):\n",
    "        \"\"\" Step 5: define optimizer \"\"\"\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss,\n",
    "                                                                                        global_step=self.global_step)\n",
    "\n",
    "    @define_scope\n",
    "    def Summaries(self):\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        tf.summary.histogram(\"histogram_loss\", self.loss)\n",
    "        # because you have several summaries, we should merge them all\n",
    "        # into one op to make it easier to manage\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "            \n",
    "    def Train(self):\n",
    "        \"\"\" Training \"\"\"\n",
    "        \n",
    "        # https://stackoverflow.com/questions/37337728/tensorflow-internalerror-blas-sgemm-launch-failed\n",
    "        if 'session' in locals() and session is not None:\n",
    "            print('Close interactive session')\n",
    "            session.close()\n",
    "        \n",
    "        saver = tf.train.Saver() # defaults to saving all variables\n",
    "        initial_step = 0\n",
    "#         with tf.device(\"/gpu:0\"):\n",
    "        with tf.Session() as sess:\n",
    "            # initialize the necessary variables, in this case, w and b\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('/tmp/tb/word2vec/checkpoints/checkpoint'))\n",
    "            # if that checkpoint exists, restore from checkpoint\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "            writer = tf.summary.FileWriter(\"/tmp/tb/word2vec/improved_graph/lr\"+str(self.learning_rate), \n",
    "                                           sess.graph)\n",
    "\n",
    "            total_loss = 0.0 # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "            loss_batch = 0\n",
    "\n",
    "            initial_step = self.global_step.eval()\n",
    "            for epoch in tqdm(xrange(initial_step, self.epochs - initial_step)):\n",
    "\n",
    "                center_word_indices, target_word_indices = self.batch_generator.next()\n",
    "                feed_dict={self.batch_inputs: center_word_indices, self.batch_labels: target_word_indices}\n",
    "\n",
    "\n",
    "\n",
    "                # to deal with  summary_op issue\n",
    "                # https://stackoverflow.com/questions/38243194/tensorflow-feed-dict-error-you-must-feed-a-value-for-placeholder-tensor\n",
    "                if epoch % SKIP_STEP == 0:\n",
    "                    loss_batch, _, final_embedding_matrix, summary = sess.run([self.loss, self.optimizer,\n",
    "                                                                               self.normalized_embedding_matrix,\n",
    "                                                                               self.summary_op],\n",
    "                                                                              feed_dict=feed_dict)\n",
    "\n",
    "                    writer.add_summary(summary, global_step=epoch)\n",
    "                else:\n",
    "                    loss_batch, _, final_embedding_matrix = sess.run([self.loss, self.optimizer,\n",
    "                                                                      self.normalized_embedding_matrix],\n",
    "                                                                     feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "                total_loss += loss_batch\n",
    "                if (epoch + 1) % SKIP_STEP == 0:\n",
    "                    tqdm.write('Average loss at step {}: {:5.1f}'.format(epoch, total_loss / SKIP_STEP))\n",
    "                    total_loss = 0.0\n",
    "                    saver.save(sess, \"/tmp/tb/word2vec/checkpoints/skip-gram\", epoch)\n",
    "\n",
    "            # code to visualize the embeddings\n",
    "            # it has to variable. constants don't work here.\n",
    "            embedding_var = tf.Variable(final_embedding_matrix[:1000], name='embedding')\n",
    "            sess.run(embedding_var.initializer)\n",
    "\n",
    "            config = projector.ProjectorConfig()\n",
    "            summary_writer = tf.summary.FileWriter('/tmp/tb/word2vec/visualize')\n",
    "\n",
    "            # add embedding to the config file\n",
    "            embedding = config.embeddings.add()\n",
    "            embedding.tensor_name = embedding_var.name\n",
    "\n",
    "            # link this tensor to its metadata file, in this case the first 500 words of vocab\n",
    "            embedding.metadata_path = './vocab_1000.tsv'\n",
    "\n",
    "            # saves a configuration file that TensorBoard will read during startup.\n",
    "            projector.visualize_embeddings(summary_writer, config)\n",
    "            saver_embed = tf.train.Saver()\n",
    "            saver_embed.save(sess, '/tmp/tb/word2vec/visualize.ckpt', 1)\n",
    "        \n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from process_data import process_data\n",
    "batch_generator = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 20] Not a directory: '/tmp/tb/word2vec/checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a2d2f3b8a480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSkipGramModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mskipgram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkipGramModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mskipgram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bicepjai/Projects/myclasses/2017/cs20si/assignments/exercises/word2vec.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/tb/word2vec/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/shutil.pyc\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mfullname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bicepjai/Programs/anaconda2/envs/deepl/lib/python2.7/shutil.pyc\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 20] Not a directory: '/tmp/tb/word2vec/checkpoint'"
     ]
    }
   ],
   "source": [
    "from word2vec import SkipGramModel\n",
    "skipgram = SkipGramModel(batch_generator, EPOCHS, LEARNING_RATE)\n",
    "skipgram.Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
